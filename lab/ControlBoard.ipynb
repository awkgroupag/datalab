{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlboard for Data Analytics-Stacks\n",
    "* [Jupyter-Datascience-Notebook](#Jupyter-Datascience-Notebook)    \n",
    "* [Very helpful: Manipulate your Docker environment](#Manipulate-your-Docker-environment)\n",
    "\n",
    "Available stacks to plug into Jupyter:\n",
    "* [Elastic Stack (formerly ELK-Stack)](#Elastic-Stack-(formerly-ELK-Stack))\n",
    "* [PostgreSQL Database, using SQLAlchemy](#PostgreSQL-Database-using-SQLAlchemy)\n",
    "* [MySQL Database, using SQLAlchemy](#MySQL)\n",
    "* [Neo4j](#Neo4j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Set-up myvalues.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set some variables first - be sure to run this code!\n",
    ">Note: you can either set some variables here, or you will have to set them in each cell below!\n",
    "\n",
    "Edit the variable definitions in the next cell to fit your needs.\n",
    "* `PROJECT_NAME`: name of this project. Will show up in all container names associated with this project. No spaces or special characters allowed\n",
    "* `DATALAB_SOURCECODE_DIR`: your Windows directory containing all your source code - including this datalab! Will appear as `/home/jovyan/work` in the Jupyter Notebook\n",
    "* `DATALAB_DATA_DIR`: your Windows directory containing all data. Will be mounted as `/home/jovyan/data` in the Notebook\n",
    "\n",
    ">Note: **Only data in either directory will survive the destruction of the Jupyter Notebook container!**\n",
    "\n",
    ">Note: your Windows paths must be written in UNIX style .. use `wsl -e wslpath \"c:\\users\\kat\\projekte\"` to\n",
    "> translate your path into the valid `/mnt/c/users/kat/projekte`\n",
    "\n",
    "\n",
    "Once your done editing in the cell below, run the code (Ctrl+Enter inside the cell):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Initialization\n",
    "Always run this code before doing anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import kubernetes\n",
    "import secrets\n",
    "import base64\n",
    "from modules import k8swrapper as k8s\n",
    "\n",
    "kubernetes.config.load_incluster_config()\n",
    "api = kubernetes.client.CoreV1Api()\n",
    "\n",
    "# For additional Helm charts like postgresql\n",
    "! helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myvalues_path = './myvalues.yaml'\n",
    "\n",
    "with open(myvalues_path) as f:\n",
    "    myvalues = yaml.safe_load(f.read())\n",
    "namespace = myvalues['namespace']\n",
    "jupyter_release_name = myvalues['jupyterReleaseName']\n",
    "sourcecode_dir = myvalues['sourcecodeDirectory']\n",
    "data_dir = myvalues['dataDirectory']\n",
    "\n",
    "print(f'Using Kubernetes namespace {namespace}, jupyterReleaseName {jupyter_release_name}')\n",
    "print(f'Sourcecode directory: {sourcecode_dir}')\n",
    "print(f'Data directory: {data_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Jupyter Data Science-Notebook\n",
    "### Start a Jupyter Kubernetes Pod\n",
    "If you this command does not provide an URL, simply re-run the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! helm upgrade --install -f $myvalues_path $jupyter_release_name jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Jupyter Pod\n",
    "This will remove the Jupyter pod (stopping is not possible with Kubernetes). Only data in the ```sourcecode``` and ```data directory``` will be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! helm delete $jupyter_release_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to save your entire computational context if you installed additional packages\n",
    "You might change your pod by installing new **PIP** Python packages e.g. with `pip install <package name>`. Any such change will be lost with the pod. To quickly save your entire pip environment, including all packages, copy-paste the following into your Juypter notebook and run it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "! pip freeze > /home/jovyan/work/pip-environment.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load your environment again from scratch, e.g. if you re-created your environment/pod:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "! pip install -r /home/jovyan/work/pip-environment.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed additional Python packages with **Anaconda**, `conda install <package name>`, here's how to save the entire conda environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "! conda env export -n base > /home/jovyan/work/anaconda-environment.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-install all Anaconda packages from this file, do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "! conda env update --name base --file /home/jovyan/work/anaconda-environment.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## PostgreSQL Database using SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) is a powerful, open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\n",
    "* [SQLAlchemy](https://www.sqlalchemy.org/) is a GREAT Python wrapper to talk to almost any database.\n",
    "\n",
    "\n",
    "Check out the [Database getting started Jupyter notebook](database_getting_started.ipynb) for code snippets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Get Passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the Kubernetes secret\n",
    "postgresql_secret = 'postgresql'\n",
    "\n",
    "postgresql_passwords = {\n",
    "    # Admin password\n",
    "    'postgres-password': k8s.alphanumeric_password(16),\n",
    "    # Password for the normal user\n",
    "    'password': k8s.alphanumeric_password(16),\n",
    "    # Replication password for user repl_user\n",
    "    'replication-password': k8s.alphanumeric_password(16)\n",
    "}\n",
    "postgresql_passwords = k8s.create_or_get_secret(api, postgresql_secret, namespace, postgresql_passwords)\n",
    "print(f'Password for admin \"postgres\": {postgresql_passwords[\"postgres-password\"]}\\nPassword for user: {postgresql_passwords[\"password\"]}\\nPassword for repl_user: {postgresql_passwords[\"replication-password\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_release = 'postgresql'\n",
    "version = '11.6.25'\n",
    "\n",
    "! helm upgrade --install --version $version --set auth.existingSecret=$postgresql_secret $postgresql_release bitnami/postgresql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! helm delete $postgresql_release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup and delete database data\n",
    "The command above will leave both the Kubernetes Secret and a PVC `data-postgresql-0` (where your PostgreSQL data is stored!). To delete them, manually type:\n",
    "- `! kubectl delete secret $postgresql_secret`\n",
    "- `! kubectl delete pvc data-postgresql-0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl delete pvc data-postgresql-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the Stack\n",
    "Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Rancher Desktop is able to mount local paths into the Kubernetes cluster, you simply will have to create a directory called `<PROJECT_NAME>-postgres` inside your data directory (i.e. DATALAB_DATA_DIR) **before** you start the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DATALAB_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME= 'test2'\n",
    "\n",
    "k8s.create_project('postgres', PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR,\n",
    "                   postgresPort='5432')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the actual database and thus all Postgres data\n",
    "\n",
    "\n",
    "Simply remove the specific directory `<PROJECT_NAME>-postgres` from your data directory (i.e. DATALAB_DATA_DIR) on your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directoryToBeRemoved = DATALAB_DATA_DIR + '/' + PROJECT_NAME + '-postgres'\n",
    "print('Remove the directory .. \"' + directoryToBeRemoved + '\" on your notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Elastic Stack (formerly ELK-Stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Elasticsearch, Kibana, Beats, and Logstash. Take data from any source, in any format, then search, analyze, and visualize it in real time.\n",
    "\n",
    "* **Elasticsearch** is a distributed, RESTful search and analytics engine. As the heart of the Elastic Stack, it centrally stores your data for lightning fast search, fine‑tuned relevancy, and powerful analytics that scale with ease.\n",
    "* **Kibana** lets you visualize your Elasticsearch data and navigate the Elastic Stack so you can do anything from tracking query load to understanding the way requests flow through your apps.\n",
    "* **Logstash** is a server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite \"stash.\"\n",
    "* **Beats** is the platform for single-purpose data shippers. They send data from hundreds or thousands of machines and systems to Logstash.\n",
    "\n",
    "Note that Beats (e.g. Metricbeat or Systembeat) are not included in this stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connections once the stack has been started\n",
    "* Direct Kibana browser access: `https://localhost/<PROJECT_NAME>-elastic`\n",
    "* Elasticsearch access for Windows: [http://localhost:9200](http://localhost:9200)\n",
    "* Access Elasticsearch from **within** a Jupyter container: [http://elasticsearch:9200](http://elasticsearch:9200)\n",
    "* Logstash access from **within** a Jupyter container: [http://logstash:9600](http://elasticsearch:9600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Rancher Desktop is able to mount local paths into the Kubernetes cluster, you simply will have to create a directory called `<PROJECT_NAME>-elastic` inside your data directory (i.e. DATALAB_DATA_DIR) **before** you start the ELK stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DATALAB_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the stack\n",
    "To start the stack, execute the next cell. \n",
    "If necessary, you can change the port settings for Elastic, Logstash and Kibana. This is only necessary if you are NOT using the defaults:\n",
    "```\n",
    "    ELASTICSEARCH_PORT=9200 (API)\n",
    "    LOGSTASH_PORT1=5000 (Beat)\n",
    "    LOGSTASH_PORT2=9600 (Monitoring)\n",
    "    KIBANA_PORT=5601\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME= 'test1'\n",
    "\n",
    "k8s.create_project('elk', PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR,\n",
    "                   elasticPort='9200', logstashPort1='5000', logstashPort2='9600', kibanaPort='5601')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once pull has completed and containers are running, startup might take 1-2 minutes!\n",
    "\n",
    "In order to complete the setup, you need to configure the default user for Kibana in Elasticsearch. To do this, run the following command in a shell on your notebook. Make sure to replace `<PROJECT_NAME>` with the name of your project before running the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec <PROJECT_NAME>-elastic -c elasticsearch -- bin/elasticsearch-create-enrollment-token --scope kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get an enrollment token in the form `eyJ2ZXIiOiI4L...1RHcifQ==` that is needed to connect Kibana with Elastic.\n",
    "\n",
    "Next, get the 6-digit code from Kibana to authenticate the first time. This can be done with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Kibana setup URL incl. code\n",
    "k8s.get_kibana_setup_url('test1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, open your browser at the URL given above (e.g. `http://localhost/test1-kibana/?code=123123`) and paste the enrollment token into the form. Press enter. This will connect Kibana with Elasticsearch and finish the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the following command in a shell on your notebook. This will create a user named \"superuser\" with password \"admin1\" and the role \"superuser\". You can use this user to login to Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec <PROJECT_NAME>-elastic -c elasticsearch -- bin/elasticsearch-users useradd superuser -p admin1 -r superuser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop and remove the stack (Elasticsearch and Kibana data will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all Elasticsearch and Kibana data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply remove the specific directory `<PROJECT_NAME>-elastic` from your data directory (i.e. DATALAB_DATA_DIR) on your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryToBeRemoved = DATALAB_DATA_DIR + '/' + PROJECT_NAME + '-elastic'\n",
    "print('Remove the directory .. \"' + directoryToBeRemoved + '\" on your notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# MySQL\n",
    "* [MySQL](https://www.mysql.com) is another popular database.\n",
    "* [SQLAlchemy](https://www.sqlalchemy.org/) is a GREAT Python wrapper to talk to almost any database.\n",
    "\n",
    "Check out the [Database getting started Jupyter notebook](database_getting_started.ipynb) for code snippets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the stack\n",
    "Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Rancher Desktop is able to mount local paths into the Kubernetes cluster, you simply will have to create a directory called `<PROJECT_NAME>-mysql` inside your data directory (i.e. DATALAB_DATA_DIR) **before** you start the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DATALAB_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME= 'test3'\n",
    "\n",
    "k8s.create_project('mysql', PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR,\n",
    "                   mySqlPort='3306')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the actual database and thus all MySQL data\n",
    "\n",
    "Simply remove the specific directory `<PROJECT_NAME>-mysql` from your data directory (i.e. DATALAB_DATA_DIR) on your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryToBeRemoved = DATALAB_DATA_DIR + '/' + PROJECT_NAME + '-mysql'\n",
    "print('Remove the directory .. \"' + directoryToBeRemoved + '\" on your notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Neo4j\n",
    "[Neo4j](https://neo4j.com/) is the leading graph database platform. The two plugins [APOC](https://neo4j.com/developer/neo4j-apoc/) and [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/) are included in the stack. All data is saved into a new directory `neo4j` in your `DATALAB_DATA_DIR`.\n",
    "* Neo4j web GUI: http://localhost:7474\n",
    "* Bolt access: http://localhost:7687\n",
    "\n",
    "Neo4j features powerful plugins. You probably want to download [Awesome Procedures APOC](https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases) and/or the [Graph Data Science Library](https://github.com/neo4j/graph-data-science/releases). Simply save the `*.tar` file into the folder `./datalab-stacks/neo4j/plugins` **BEFORE** you start the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the Stack\n",
    "As Rancher Desktop is able to mount local paths into the Kubernetes cluster, you simply will have to create a directory called `<PROJECT_NAME>-neo4j` inside your data directory (i.e. DATALAB_DATA_DIR) **before** you start the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DATALAB_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stack. Note: we assume that you saved the entire datalab in a subfolder `datalab` of your `DATALAB_SOURCECODE_DIR` for plugins to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME= 'test4'\n",
    "\n",
    "k8s.create_project('neo4j', PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR,\n",
    "                   neo4jHttpPort='7474', neo4jBoltPort='7687')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the actual database and thus all Neo4j data\n",
    "\n",
    "Simply remove the specific directory `<PROJECT_NAME>-neo4j` from your data directory (i.e. DATALAB_DATA_DIR) on your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryToBeRemoved = DATALAB_DATA_DIR + '/' + PROJECT_NAME + '-neo4j'\n",
    "print('Remove the directory .. \"' + directoryToBeRemoved + '\" on your notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Manipulate your Kubernetes environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all existing pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import k8swrapper as k8s\n",
    "\n",
    "# list pods in default namespace \"default\"\n",
    "k8s.list_pods(k8s.get_pods_namespace())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all Docker images including their filesizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all volumes (=data volumes if you choose to not mount a Windows directory, for example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In desperate need to figure out what's eating up your disk space? This command shows where Docker is using disk space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker system df -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate a container\n",
    "Set a container name (or CONTAINER ID) first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"jupyter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker stop $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the running container's logs saved to the Python variable `logoutput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logoutput = ! sudo docker logs $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart an existing (currently stopped) container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker start $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the container completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker rm $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up and freeing disk space\n",
    "Remove an image (give either it's name or IMAGE ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"test\"\n",
    "! sudo docker image rm $image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopped containers at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker container prune --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove a volume (=data volume, thus potentially deleting your data!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = \"test\"\n",
    "! sudo docker volume rm $volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Danger zone**: remove all stopped containers, and all images and all volumes that are currently not associated/mounted with a **running container**. Type the following manually:\n",
    "* Delete all stopped containers, all \"dangling\" images, the build cache, any unattached network: ```! sudo docker system prune --force```\n",
    "* To also delete all currently unused images: ```! sudo docker system prune --all --force```\n",
    "* To also delete all currently unused volumes (potentially deleting your data!): ```! sudo docker system prune --volumes --force```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
