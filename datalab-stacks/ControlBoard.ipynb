{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlboard for Data Analytics-Stacks\n",
    "* [Jupyter-Datascience-Notebook](#Jupyter-Datascience-Notebook)    \n",
    "* [Very helpful: Manipulate your Docker environment](#Manipulate-your-Docker-environment)\n",
    "\n",
    "Available stacks to plug into Jupyter:\n",
    "* [Elastic Stack (formerly ELK-Stack)](#Elastic-Stack-(formerly-ELK-Stack))\n",
    "* [PostgreSQL Database, using SQLAlchemy](#PostgreSQL-Database-using-SQLAlchemy)\n",
    "* [MySQL Database, using SQLAlchemy](#MySQL)\n",
    "* [Neo4j](#Neo4j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Jupyter Datascience-Notebook\n",
    "#### Summary\n",
    "\n",
    "Your \"standard\" [Jupyter Notebook for Datascience](https://github.com/jupyter/docker-stacks/tree/master/datascience-notebook) plus some additional libraries and the Jupyterlab extensions\n",
    "* [jupyterlab-git: git and GitHub integration](https://github.com/jupyterlab/jupyterlab-git)\n",
    "* [jupyterlab-lsp: Code completion, function definition look-up and more](https://github.com/krassowski/jupyterlab-lsp)\n",
    "* [Code debugger](https://github.com/jupyterlab/debugger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set some variables first - be sure to run this code!\n",
    ">Note: you can either set some variables here, or you will have to set them in each cell below!\n",
    "\n",
    "Edit the variable definitions in the next cell to fit your needs.\n",
    "* `PROJECT_NAME`: name of this project. Will show up in all container names associated with this project. No spaces or special characters allowed\n",
    "* `DATALAB_SOURCECODE_DIR`: your Windows directory containing all your source code - including this datalab! Will appear as `/home/jovyan/work` in the Jupyter Notebook\n",
    "* `DATALAB_DATA_DIR`: your Windows directory containing all data. Will be mounted as `/home/jovyan/data` in the Notebook\n",
    "\n",
    ">Note: **Only data in either directory will survive the destruction of the Jupyter Notebook container!**\n",
    "\n",
    ">Note: your Windows paths must be written in UNIX style .. use `wsl -e wslpath \"c:\\users\\kat\\projekte\"` to\n",
    "> translate your path into the valid `/mnt/c/users/kat/projekte`\n",
    "\n",
    "\n",
    "Once your done editing in the cell below, run the code (Ctrl+Enter inside the cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the k8s wrapper module which abstracts calls to the Kubernetes Python library\n",
    "from modules import k8swrapper as k8s\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# project specific settings\n",
    "\n",
    "# unique project name .. needed for configuration files and access \n",
    "PROJECT_NAME = 'test1'\n",
    "# data dir can be shared between notebooks - this is where the configuration file\n",
    "# for this notebook is stored, too\n",
    "DATALAB_DATA_DIR = '/mnt/c/Users/Baw/OneDrive - AWK Group AG/projekte/docker_mount/awk_datalab/data'\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# setup specific settings .. stay mostly the same.\n",
    "# change the next entry to point to the AWK Datalab source dir\n",
    "DATALAB_SOURCECODE_DIR = '/mnt/c/Users/Baw/OneDrive - AWK Group AG/projekte/docker_mount/awk_datalab/datalab'\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# you probably do not need to change anything below here, as they remain largely the same for all notebooks\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "print('These are your settings:')\n",
    "print('')\n",
    "print('Projectname:\\t' + PROJECT_NAME)\n",
    "print('Data directory:\\t' + DATALAB_DATA_DIR)\n",
    "print('Work directory:\\t' + DATALAB_SOURCECODE_DIR)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the container\n",
    "Next, to start a single Jupyter Notebook with your settings above, just run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k8s.create_project('jupyter',PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, get the URL to access the notebook in your browser\n",
    ">Note: k8s.get_project_url() tries 5 times to get the URL from the logfile .. and waits 3 seconds in between\n",
    ">if it fails, it returns \"Not ready yet, please try again.\"\n",
    ">if it succeeds, the URL to access the Jupyter Notebook is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Open your project in your browser with the following URL:')\n",
    "print(k8s.get_project_url(PROJECT_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopping and cleaning up\n",
    "Stop the Jupyter Notebook and delete the pod. The data will be retained in your DATALAB_DATA_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might well be that terminating a pod and its container(s) takes a while .. just list the pods once again until the deleted pod is gone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to clean up all the data and the configuration file for the projext (named <PROJECT_NAME>.yml, and stored in the DATALAB_DAZTA_DIR), then just remove it after you deleted the project as instructed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to save your entire computational context if you installed additional packages\n",
    "You might change your container by installing new **PIP** Python packages e.g. with `pip install <package name>`. This change will be lost with the container. To quickly save your entire pip environment, including all packages, copy-paste the following into your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > /home/jovyan/work/pip-environment.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load your environment again from scratch, e.g. if you re-created your container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r /home/jovyan/work/pip-environment.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed additional Python packages with **Anaconda**, `conda install <package name>`, here's how to save the entire conda environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env export -n base > /home/jovyan/work/anaconda-environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-install all Anaconda packages from this file, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env update --name base --file /home/jovyan/work/anaconda-environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Elastic Stack (formerly ELK-Stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Elasticsearch, Kibana, Beats, and Logstash. Take data from any source, in any format, then search, analyze, and visualize it in real time.\n",
    "\n",
    "* **Elasticsearch** is a distributed, RESTful search and analytics engine. As the heart of the Elastic Stack, it centrally stores your data for lightning fast search, fineâ€‘tuned relevancy, and powerful analytics that scale with ease.\n",
    "* **Kibana** lets you visualize your Elasticsearch data and navigate the Elastic Stack so you can do anything from tracking query load to understanding the way requests flow through your apps.\n",
    "* **Logstash** is a server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite \"stash.\"\n",
    "* **Beats** is the platform for single-purpose data shippers. They send data from hundreds or thousands of machines and systems to Logstash.\n",
    "\n",
    "Note that Beats (e.g. Metricbeat or Systembeat) are not included in this stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connections once the stack has been started\n",
    "* Direct Kibana browser access: `https://localhost/<PROJECT_NAME>-elastic`\n",
    "* Elasticsearch access for Windows: [http://localhost:9200](http://localhost:9200)\n",
    "* Access Elasticsearch from **within** a Jupyter container: [http://elasticsearch:9200](http://elasticsearch:9200)\n",
    "* Logstash access from **within** a Jupyter container: [http://logstash:9600](http://elasticsearch:9600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Rancher Desktop is able to mount local paths into the Kubernetes cluster, you simply will have to create a directory called `<PROJECT_NAME>-elastic` inside your data directory (i.e. DATALAB_DATA_DIR) **before** you start the ELK stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DATALAB_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the stack\n",
    "To start the stack, execute the next cell. \n",
    "If necessary, you can change the port settings for Elastic, Logstash and Kibana. This is only necessary if you are NOT using the defaults:\n",
    "```\n",
    "    ELASTICSEARCH_PORT=9200 (API)\n",
    "    LOGSTASH_PORT1=5000 (Beat)\n",
    "    LOGSTASH_PORT2=9600 (Monitoring)\n",
    "    KIBANA_PORT=5601\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME= 'test1'\n",
    "\n",
    "k8s.create_project('elk', PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR,\n",
    "                   elasticPort='9200', logstashPort1='5000', logstashPort2='9600', kibanaPort='5601')\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once pull has completed and containers are running, startup might take 1-2 minutes!\n",
    "\n",
    "In order to complete the setup, you need to configure the default user for Kibana in Elasticsearch. To do this, run the following command in a shell on your notebook. Make sure to replace `<PROJECT_NAME>` with the name of your project before running the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec <PROJECT_NAME>-elastic -c elasticsearch -- bin/elasticsearch-create-enrollment-token --scope kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get an enrollment token in the form `eyJ2ZXIiOiI4L...1RHcifQ==` that is needed to connect Kibana with Elastic.\n",
    "\n",
    "Next, get the 6-digit code from Kibana to authenticate the first time. This can be done with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go to https://localhost/test1-kibana/?code=037625 to get started.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Kibana setup URL incl. code\n",
    "k8s.get_kibana_setup_url('test1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, open your browser at the URL given above (e.g. `http://localhost/test1-kibana/?code=123123`) and paste the enrollment token into the form. Press enter. This will connect Kibana with Elasticsearch and finish the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the following command in a shell on your notebook. This will create a user named \"superuser\" with password \"admin1\" and the role \"superuser\". You can use this user to login to Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec <PROJECT_NAME>-elastic -c elasticsearch -- bin/elasticsearch-users useradd superuser -p admin1 -r superuser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop and remove the stack (Elasticsearch and Kibana data will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all Elasticsearch and Kibana data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply remove the specific directory `<PROJECT_NAME>-elastic` from your data directory (i.e. DATALAB_DATA_DIR) on your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryToBeRemoved = DATALAB_DATA_DIR + '/' + PROJECT_NAME + '-elastic'\n",
    "print('Remove the directory .. \"' + directoryToBeRemoved + '\" on your notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## PostgreSQL Database using SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) is a powerful, open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\n",
    "* [SQLAlchemy](https://www.sqlalchemy.org/) is a GREAT Python wrapper to talk to almost any database.\n",
    "\n",
    "\n",
    "Check out the [Database getting started Jupyter notebook](database_getting_started.ipynb) for code snippets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the Stack\n",
    "Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Rancher Desktop is able to mount local paths into the Kubernetes cluster, you simply will have to create a directory called `<PROJECT_NAME>-postgres` inside your data directory (i.e. DATALAB_DATA_DIR) **before** you start the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DATALAB_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME= 'test2'\n",
    "\n",
    "k8s.create_project('postgres', PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR,\n",
    "                   postgresPort='5432')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the actual database and thus all Postgres data\n",
    "\n",
    "\n",
    "Simply remove the specific directory `<PROJECT_NAME>-postgres` from your data directory (i.e. DATALAB_DATA_DIR) on your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryToBeRemoved = DATALAB_DATA_DIR + '/' + PROJECT_NAME + '-postgres'\n",
    "print('Remove the directory .. \"' + directoryToBeRemoved + '\" on your notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# MySQL\n",
    "* [MySQL](https://www.mysql.com) is another popular database.\n",
    "* [SQLAlchemy](https://www.sqlalchemy.org/) is a GREAT Python wrapper to talk to almost any database.\n",
    "\n",
    "Check out the [Database getting started Jupyter notebook](database_getting_started.ipynb) for code snippets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the stack\n",
    "Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Rancher Desktop is able to mount local paths into the Kubernetes cluster, you simply will have to create a directory called `<PROJECT_NAME>-mysql` inside your data directory (i.e. DATALAB_DATA_DIR) **before** you start the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DATALAB_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME= 'test3'\n",
    "\n",
    "k8s.create_project('mysql', PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR,\n",
    "                   mySqlPort='3306')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the actual database and thus all MySQL data\n",
    "\n",
    "Simply remove the specific directory `<PROJECT_NAME>-mysql` from your data directory (i.e. DATALAB_DATA_DIR) on your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryToBeRemoved = DATALAB_DATA_DIR + '/' + PROJECT_NAME + '-mysql'\n",
    "print('Remove the directory .. \"' + directoryToBeRemoved + '\" on your notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Neo4j\n",
    "[Neo4j](https://neo4j.com/) is the leading graph database platform. The two plugins [APOC](https://neo4j.com/developer/neo4j-apoc/) and [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/) are included in the stack. All data is saved into a new directory `neo4j` in your `DATALAB_DATA_DIR`.\n",
    "* Neo4j web GUI: http://localhost:7474\n",
    "* Bolt access: http://localhost:7687\n",
    "\n",
    "Neo4j features powerful plugins. You probably want to download [Awesome Procedures APOC](https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases) and/or the [Graph Data Science Library](https://github.com/neo4j/graph-data-science/releases). Simply save the `*.tar` file into the folder `./datalab-stacks/neo4j/plugins` **BEFORE** you start the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the Stack\n",
    "As Rancher Desktop is able to mount local paths into the Kubernetes cluster, you simply will have to create a directory called `<PROJECT_NAME>-neo4j` inside your data directory (i.e. DATALAB_DATA_DIR) **before** you start the Postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (DATALAB_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stack. Note: we assume that you saved the entire datalab in a subfolder `datalab` of your `DATALAB_SOURCECODE_DIR` for plugins to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME= 'test4'\n",
    "\n",
    "k8s.create_project('neo4j', PROJECT_NAME, DATALAB_DATA_DIR, DATALAB_SOURCECODE_DIR,\n",
    "                   neo4jHttpPort='7474', neo4jBoltPort='7687')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete project in default namespace \"default\"\n",
    "k8s.delete_project('test4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the actual database and thus all Neo4j data\n",
    "\n",
    "Simply remove the specific directory `<PROJECT_NAME>-neo4j` from your data directory (i.e. DATALAB_DATA_DIR) on your laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryToBeRemoved = DATALAB_DATA_DIR + '/' + PROJECT_NAME + '-neo4j'\n",
    "print('Remove the directory .. \"' + directoryToBeRemoved + '\" on your notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Manipulate your Kubernetes environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all existing pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import k8swrapper as k8s\n",
    "\n",
    "# list pods in default namespace \"default\"\n",
    "k8s.list_pods(k8s.get_pods_namespace())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all Docker images including their filesizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all volumes (=data volumes if you choose to not mount a Windows directory, for example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In desperate need to figure out what's eating up your disk space? This command shows where Docker is using disk space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker system df -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate a container\n",
    "Set a container name (or CONTAINER ID) first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"jupyter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker stop $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the running container's logs saved to the Python variable `logoutput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logoutput = ! sudo docker logs $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart an existing (currently stopped) container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker start $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the container completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker rm $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up and freeing disk space\n",
    "Remove an image (give either it's name or IMAGE ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"test\"\n",
    "! sudo docker image rm $image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopped containers at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker container prune --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove a volume (=data volume, thus potentially deleting your data!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = \"test\"\n",
    "! sudo docker volume rm $volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Danger zone**: remove all stopped containers, and all images and all volumes that are currently not associated/mounted with a **running container**. Type the following manually:\n",
    "* Delete all stopped containers, all \"dangling\" images, the build cache, any unattached network: ```! sudo docker system prune --force```\n",
    "* To also delete all currently unused images: ```! sudo docker system prune --all --force```\n",
    "* To also delete all currently unused volumes (potentially deleting your data!): ```! sudo docker system prune --volumes --force```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
